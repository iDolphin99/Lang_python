{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ea398b",
   "metadata": {},
   "source": [
    "# Notice \n",
    "\n",
    "본 jupyter notebook 파일은 \"\" 최종 프로젝트 과제 코드입니다.  \n",
    "셀 순서대로 실행하시면 됩니다. \n",
    " \n",
    "- 프로젝트 제목: 해외 뉴스 매체 기반 영어 단어장 만들기  \n",
    "- 작성자: \"dolphin\"\n",
    "- 참고자료\n",
    "    - [출처] 파이썬#70 - 파이썬 크롤링과 nltk 를 활용한 영어단어 퀴즈 게임|작성자 남박사(https://blog.naver.com/nkj2001/222713831884)\n",
    "    - [출처] Python을 통한 BBC 뉴스 크롤링하기(https://1eed00.tistory.com/175)\n",
    "    - [출처] 딥 러닝을 이용한 자연어 처리 입문(https://wikidocs.net/22530)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f623c8",
   "metadata": {},
   "source": [
    "# 0. Libraries\n",
    "\n",
    "본 프로젝트에서는 아래의 Python library를 사용하였습니다.  \n",
    "필요에 의해 !pip install 구문으로 설치할 수 있습니다.  \n",
    "nltk 라이브러리의 경우 원활하게 사용하기 위해 자연어 데이터를 다운로드해야 합니다. 다운로드 코드는 주석 처리를 해두었습니다. \n",
    "\n",
    "- Requests, BeautifulSoup (크롤링을 위해)\n",
    "- nltk (자연어 데이터 처리를 위해)\n",
    "- Pandas (데이터 처리를 위해)\n",
    "- re, random, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f25389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk \n",
    "# nltk.download() # nltk data를 다운받기 위한 코드\n",
    "import pandas as pd\n",
    "import re, random, string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9effa",
   "metadata": {},
   "source": [
    "# 1. International News Crawling \n",
    "해외 뉴스 매체 3곳(ABC, BBC, CNN)으로부터 영단어를 크롤링합니다. \n",
    "- abc_df: ABC News 데이터\n",
    "- bbc_df: BBC News 데이터\n",
    "- cnn_df: CNN News 데이터\n",
    "- all_df: abc_df + bbc_df + cnn_df (크롤링된 News 데이터를 모두 모은 데이터프레임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd648f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 매체 URL 설정\n",
    "abc_url = 'https://abcnews.go.com/International'\n",
    "bbc_url = 'http://feeds.bbci.co.uk/news/rss.xml'\n",
    "cnn_url = 'https://www.cnn.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1700154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헤더 설정\n",
    "header = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0e7d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN news 메인 페이지에서 기사 링크에 접근하는 과정에서 쓰이는 함수\n",
    "def url_is_article(url, current_year='2023'):\n",
    "    if url:\n",
    "        if 'cnn.com/{}/'.format(current_year) in url and '/gallery/' not in url:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 크롤링 과정 중 접근한 요소에 text 유무를 탐지하는 함수 (error handling)\n",
    "def return_text_if_not_none(element):\n",
    "    if element:\n",
    "        return element.text.strip()\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc8800",
   "metadata": {},
   "source": [
    "### ABC News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72a8703d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Article Text</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Missing at age 11, found six years later in Fr...</td>\n",
       "      <td>There's still a heap of unanswered questions, ...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardinal sentenced to 5 1/2 years in Vatican's...</td>\n",
       "      <td>Cardinal Becciu and 8 others were convicted of...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vatican 'trial of the century,' a Pandora's bo...</td>\n",
       "      <td>It is the most complicated financial trial in ...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61 migrants drown in 'shipwreck' off Libyan co...</td>\n",
       "      <td>The vessel left Libya with about 86 people onb...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Indian Navy is shadowing a bulk carrier li...</td>\n",
       "      <td>The Indian Navy says it is shadowing a bulk ca...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Missing at age 11, found six years later in Fr...   \n",
       "1  Cardinal sentenced to 5 1/2 years in Vatican's...   \n",
       "2  Vatican 'trial of the century,' a Pandora's bo...   \n",
       "3  61 migrants drown in 'shipwreck' off Libyan co...   \n",
       "4  The Indian Navy is shadowing a bulk carrier li...   \n",
       "\n",
       "                                        Article Text News  \n",
       "0  There's still a heap of unanswered questions, ...  ABC  \n",
       "1  Cardinal Becciu and 8 others were convicted of...  ABC  \n",
       "2  It is the most complicated financial trial in ...  ABC  \n",
       "3  The vessel left Libya with about 86 people onb...  ABC  \n",
       "4  The Indian Navy says it is shadowing a bulk ca...  ABC  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스 메인 페이지 접근\n",
    "req = requests.get(abc_url, headers=header)\n",
    "bs = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "# 뉴스 메인에 있는 기사 링크에 하나 씩 접근\n",
    "all_data = []\n",
    "for link in bs.select(\"div.ContentList__Item > a\"): \n",
    "    req = requests.get(link.get(\"href\"), headers=header)\n",
    "    bs = BeautifulSoup(req.text, \"html.parser\")\n",
    "    \n",
    "    # 기사 제목, 기사 내용 크롤링\n",
    "    title = return_text_if_not_none(bs.find('h1'))\n",
    "    contents = return_text_if_not_none(bs.find('div', attrs={'data-testid': 'prism-article-body'}))\n",
    "    content = [i.text.strip() for i in bs.find_all('p')]\n",
    "    content = ' '.join(content)\n",
    "\n",
    "    data = [title, content, \"ABC\"]\n",
    "    all_data.append(data)\n",
    "\n",
    "# 기사 제목, 기사 내용, 뉴스 매체 종류 -> 총 3개의 컬럼을 가진 dataframe형태로 크롤링 데이터 저장\n",
    "abc_df = pd.DataFrame(all_data, columns=['Title', 'Article Text', 'News'])\n",
    "abc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89413652",
   "metadata": {},
   "source": [
    "### BBC News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8494b236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Article Text</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michelle Mone admits she stands to benefit fro...</td>\n",
       "      <td>This video can not be played Michelle Mone: I'...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>British teen Alex Batty returns to UK after si...</td>\n",
       "      <td>British teenager Alex Batty, who was found in ...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ian Wright: Match of the Day pundit to step do...</td>\n",
       "      <td></td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why Covid is still flooring some people</td>\n",
       "      <td>What is it like to catch Covid now? It is a qu...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strictly Come Dancing 2023: Winner of glitterb...</td>\n",
       "      <td>This video can not be played Watch: Moment win...</td>\n",
       "      <td>BBC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Michelle Mone admits she stands to benefit fro...   \n",
       "1  British teen Alex Batty returns to UK after si...   \n",
       "2  Ian Wright: Match of the Day pundit to step do...   \n",
       "3            Why Covid is still flooring some people   \n",
       "4  Strictly Come Dancing 2023: Winner of glitterb...   \n",
       "\n",
       "                                        Article Text News  \n",
       "0  This video can not be played Michelle Mone: I'...  BBC  \n",
       "1  British teenager Alex Batty, who was found in ...  BBC  \n",
       "2                                                     BBC  \n",
       "3  What is it like to catch Covid now? It is a qu...  BBC  \n",
       "4  This video can not be played Watch: Moment win...  BBC  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스 메인 페이지 접근\n",
    "req = requests.get(bbc_url, headers=header)\n",
    "bs = BeautifulSoup(req.content.decode('utf-8'), \"xml\") # BBC news의 경우 xml 페이지에서 parsing하기 때문에 encoding-decoding에 유의해야 함\n",
    "\n",
    "# 뉴스 메인에 있는 기사 링크에 하나 씩 접근\n",
    "all_data = []\n",
    "for item in bs.find_all('item'): # item과 그 내의 자식요소만 필터링\n",
    "    link = item.find('guid').get_text()\n",
    "    req = requests.get(link, headers=header)\n",
    "    bs = BeautifulSoup(req.content.decode('utf-8'), \"xml\")\n",
    "    \n",
    "    # 기사 제목, 기사 내용 크롤링\n",
    "    title = item.find_all([\"title\",\"description\"])[0].get_text()\n",
    "    content = [i.text.strip() for i in bs.find_all('p')]\n",
    "    content = ' '.join(content)\n",
    "    \n",
    "    data = [title, content, \"BBC\"] \n",
    "    all_data.append(data)\n",
    "\n",
    "# 기사 제목, 기사 내용, 뉴스 매체 종류 -> 총 3개의 컬럼을 가진 dataframe형태로 크롤링 데이터 저장\n",
    "bbc_df = pd.DataFrame(all_data, columns=['Title', 'Article Text', 'News'])\n",
    "bbc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e2227",
   "metadata": {},
   "source": [
    "### CNN News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4092b9fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Article Text</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sharon Osbourne says getting surgery on her fa...</td>\n",
       "      <td>London\\nCNN\\n         — \\n    \\n\\n\\nSharon Osb...</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weeks-old government dubbed ‘anti-Māori’ as cu...</td>\n",
       "      <td>CNN\\n         — \\n    \\n\\n\\n      New Zealand’...</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hundreds of objects in British Museum defaced,...</td>\n",
       "      <td>London\\nCNN\\n         — \\n    \\n\\n\\n      Hund...</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here’s where the minimum wage will increase ne...</td>\n",
       "      <td>New York\\nCNN\\n         — \\n    \\n\\n\\n      At...</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peeling back the layers of the extraordinary v...</td>\n",
       "      <td>CNN\\n         — \\n    \\n\\n\\n      At this very...</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Sharon Osbourne says getting surgery on her fa...   \n",
       "1  Weeks-old government dubbed ‘anti-Māori’ as cu...   \n",
       "2  Hundreds of objects in British Museum defaced,...   \n",
       "3  Here’s where the minimum wage will increase ne...   \n",
       "4  Peeling back the layers of the extraordinary v...   \n",
       "\n",
       "                                        Article Text News  \n",
       "0  London\\nCNN\\n         — \\n    \\n\\n\\nSharon Osb...  CNN  \n",
       "1  CNN\\n         — \\n    \\n\\n\\n      New Zealand’...  CNN  \n",
       "2  London\\nCNN\\n         — \\n    \\n\\n\\n      Hund...  CNN  \n",
       "3  New York\\nCNN\\n         — \\n    \\n\\n\\n      At...  CNN  \n",
       "4  CNN\\n         — \\n    \\n\\n\\n      At this very...  CNN  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스 메인 페이지 접근\n",
    "req = requests.get(cnn_url).text\n",
    "bs = BeautifulSoup(req, features=\"html.parser\")\n",
    "\n",
    "# 뉴스 메인에 있는 기사 링크 수집\n",
    "all_urls = []\n",
    "for a in bs.find_all('a', href=True):\n",
    "    if a['href'] and a['href'][0] == '/' and a['href'] != '#':\n",
    "        a['href'] = cnn_url + a['href']\n",
    "    all_urls.append(a['href'])\n",
    "article_urls = [url for url in all_urls if url_is_article(url)]\n",
    "\n",
    "# 뉴스 메인에 있는 기사 링크에 하나 씩 접근\n",
    "all_data = []\n",
    "article_urls_duplicates_removed = list(set(article_urls)) # 중복 URL 제거 \n",
    "for link in article_urls_duplicates_removed:\n",
    "    req = requests.get(link).text\n",
    "    bs = BeautifulSoup(req, features=\"html.parser\")\n",
    "    \n",
    "    # 기사 제목, 기사 내용 크롤링\n",
    "    title = return_text_if_not_none(bs.find('h1', {'class': 'headline__text'}))\n",
    "    content = return_text_if_not_none(bs.find('div', {'class': 'article__content'}))\n",
    "    \n",
    "    data = [title, content, \"CNN\"] \n",
    "    all_data.append(data)\n",
    "    \n",
    "# 기사 제목, 기사 내용, 뉴스 매체 종류 -> 총 3개의 컬럼을 가진 dataframe형태로 크롤링 데이터 저장\n",
    "cnn_df = pd.DataFrame(all_data, columns=['Title', 'Article Text', 'News'])\n",
    "cnn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed85871",
   "metadata": {},
   "source": [
    "### Concate [ABC, BBC, CNN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e37c93e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수집된 ABC News 기사 수:  9\n",
      "수집된 BBC News 기사 수:  39\n",
      "수집된 CNN News 기사 수:  73\n",
      "수집된 전체 News 기사 수:  121\n"
     ]
    }
   ],
   "source": [
    "all_df = pd.concat([abc_df, bbc_df, cnn_df]).reset_index(drop=True) \n",
    "all_df = all_df.astype('string') # text형태의 데이터이기 때문에 형식 변환이 필요함\n",
    "\n",
    "print(\"수집된 ABC News 기사 수: \", len(abc_df))\n",
    "print(\"수집된 BBC News 기사 수: \", len(bbc_df))\n",
    "print(\"수집된 CNN News 기사 수: \", len(cnn_df))\n",
    "print(\"수집된 전체 News 기사 수: \", len(all_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d86525",
   "metadata": {},
   "source": [
    "# 2. Preprocessing Natural Language Data\n",
    "크롤링한 기사 데이터는 text 형태의 자연어 데이터입니다.  \n",
    "띄어쓰기, 개행 처리 등 필요하지 않은 부분들을 처리하고 nltk 라이브러리를 이용하여 단어 별로 분리하는 작업을 진행합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9537e8",
   "metadata": {},
   "source": [
    "### Basic Preprocssing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "184b8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개행 문자(엔터), 공란(스페이스바), 크롤링 과정 중 잘못 가져온 값들을 직접 대체(replace)하여 처리\n",
    "all_df['Article Text'] = all_df['Article Text'].str.split().str.join(' ')\n",
    "all_df['Article Text'] = all_df['Article Text'].apply(lambda x: str(x).replace(u'\\xa0', u''))\n",
    "all_df['Article Text'] = all_df['Article Text'].apply(lambda x: str(x).replace('\\n', ''))\n",
    "all_df['Article Text'] = all_df['Article Text'].apply(lambda x: str(x).replace('\\'', ''))\n",
    "all_df['Article Text'] = all_df['Article Text'].apply(lambda x: str(x).replace('ABC', ''))\n",
    "all_df['Article Text'] = all_df['Article Text'].apply(lambda x: str(x).replace('BBC', ''))\n",
    "all_df['Article Text'] = all_df['Article Text'].apply(lambda x: str(x).replace('CNN', ''))\n",
    "all_df['Article Text'] = all_df['Article Text'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b45eb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theres still a heap of unanswered questions, notably: Where has he been? LE PECQ, France -- The vehicles headlights silhouetted the exhausted teenager walking alone in the rain in deepest rural France, with a skateboard tucked under his arm. “I said to myself, ‘That’s strange. It’s 3 am in the morning, it’s raining, he’s all by himself on the road between two villages,\" delivery driver Fabien Accidini recounted. From there, the story gets stranger still. The youngster, it turned out, was Alex Ba\n"
     ]
    }
   ],
   "source": [
    "# 기본 전처리를 끝낸 데이터프레임에서 기사 글을 하나의 text 형태로 추출\n",
    "text = all_df['Article Text'].values\n",
    "text = ' '.join(text)\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f00f2d",
   "metadata": {},
   "source": [
    "### Preprocessing with NLTK\n",
    "1. 토크나이징(토큰화)\n",
    "2. 불용어 제거 \n",
    "3. 표제어 추출 \n",
    "4. 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a6913c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Theres', 'still', 'a', 'heap', 'of', 'unanswered', 'questions', ',', 'notably', ':', 'Where', 'has', 'he', 'been', '?', 'LE', 'PECQ', ',', 'France', '--', 'The', 'vehicles', 'headlights', 'silhouetted', 'the', 'exhausted', 'teenager', 'walking', 'alone', 'in', 'the', 'rain', 'in', 'deepest', 'rural', 'France', ',', 'with', 'a', 'skateboard', 'tucked', 'under', 'his', 'arm.', '“I', 'said', 'to', 'myself', ',', '‘That’s', 'strange.', 'It’s', '3', 'am', 'in', 'the', 'morning', ',', 'it’s', 'raining', ',', 'he’s', 'all', 'by', 'himself', 'on', 'the', 'road', 'between', 'two', 'villages', ',', \"''\", 'delivery', 'driver', 'Fabien', 'Accidini', 'recounted.', 'From', 'there', ',', 'the', 'story', 'gets', 'stranger', 'still.', 'The', 'youngster', ',', 'it', 'turned', 'out', ',', 'was', 'Alex', 'Batty', ',', 'a', '17-year-old', 'from']\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징: 텍스트에 대해 특정 기준(단어 기준)으로 문장을 나누는 것\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text) # 토큰화된 단어들은 tokens 변수에 저장\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bbb2117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Theres', 'still', 'heap', 'unanswered', 'questions', 'notably', 'LE', 'PECQ', 'France', '--', 'vehicles', 'headlights', 'silhouetted', 'exhausted', 'teenager', 'walking', 'alone', 'rain', 'deepest', 'rural', 'France', 'skateboard', 'tucked', 'arm.', '“I', 'said', '‘That’s', 'strange.', 'It’s', '3', 'morning', 'it’s', 'raining', 'he’s', 'road', 'two', 'villages', \"''\", 'delivery', 'driver', 'Fabien', 'Accidini', 'recounted.', 'story', 'gets', 'stranger', 'still.', 'youngster', 'turned', 'Alex', 'Batty', '17-year-old', 'Britain', 'missing', 'since', '2017.', 'British', 'French', 'authorities', 'confirmed', 'Friday', 'teenager', 'found', 'Accidini', 'week', 'boy', 'vanished', 'age', '11', 'mother', 'grandfather', 'took', 'meant', 'two-week', 'family', 'holiday', 'Spain.', 'Instead', 'turned', 'six-year', 'odyssey', 'Morocco', 'Spain', 'southwest', 'France', 'living', 'off-the-grid', 'life.', 'week.', 'Batty', 'suddenly', 'popped', 'back', 'radar', 'Wednesday.', 'Thats', 'Accidini', 'found', 'alone', 'remote']\n"
     ]
    }
   ],
   "source": [
    "# 불용어(stop words) 제거: 유의미하지 않은 단어(token)를 제거하는 것 (ex, 조사, 접미사, I, my, me, mine...) \n",
    "stop_words = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
    "clean_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(clean_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "56b5f549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Theres', 'still', 'heap', 'unanswered', 'question', 'notably', 'LE', 'PECQ', 'France', '--', 'vehicle', 'headlight', 'silhouetted', 'exhausted', 'teenager', 'walking', 'alone', 'rain', 'deepest', 'rural', 'France', 'skateboard', 'tucked', 'arm.', '“I', 'said', '‘That’s', 'strange.', 'It’s', '3', 'morning', 'it’s', 'raining', 'he’s', 'road', 'two', 'village', \"''\", 'delivery', 'driver', 'Fabien', 'Accidini', 'recounted.', 'story', 'get', 'stranger', 'still.', 'youngster', 'turned', 'Alex', 'Batty', '17-year-old', 'Britain', 'missing', 'since', '2017.', 'British', 'French', 'authority', 'confirmed', 'Friday', 'teenager', 'found', 'Accidini', 'week', 'boy', 'vanished', 'age', '11', 'mother', 'grandfather', 'took', 'meant', 'two-week', 'family', 'holiday', 'Spain.', 'Instead', 'turned', 'six-year', 'odyssey', 'Morocco', 'Spain', 'southwest', 'France', 'living', 'off-the-grid', 'life.', 'week.', 'Batty', 'suddenly', 'popped', 'back', 'radar', 'Wednesday.', 'Thats', 'Accidini', 'found', 'alone', 'remote']\n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출: 단어들을(tokens) 기본 사전형 단어(표제어)로 변환하는 것 (am, are, is -> be라는 표제어로 추출 가능)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "lemm_tokens = [wnl.lemmatize(token) for token in clean_tokens]\n",
    "print(lemm_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "94d2528a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링된 단어 사전 길이:  8989\n",
      "[('year', 220), ('also', 191), ('people', 180), ('say', 176), ('first', 152), ('Images', 146), ('time', 140), ('back', 128), ('family', 121), ('new', 114), ('get', 109), ('New', 102), ('right', 91), ('still', 87), ('make', 86), ('day', 84), ('world', 82), ('last', 81), ('work', 78), ('call', 77), ('company', 77), ('want', 76), ('chip', 76), ('life', 75), ('even', 75), ('death', 75), ('way', 73), ('support', 72), ('team', 71), ('think', 71), ('see', 70), ('thing', 69), ('war', 69), ('many', 69), ('Vatican', 68), ('Gaza', 68), ('win', 68), ('week', 66), ('woman', 66), ('number', 66), ('Taiwans', 66), ('state', 64), ('take', 64), ('part', 63), ('month', 62), ('show', 62), ('external', 62), ('come', 61), ('home', 60), ('secret', 60), ('Israel', 59), ('game', 59), ('Hamas', 58), ('much', 57), ('talk', 56), ('France', 55), ('really', 55), ('sauce', 55), ('superstardom', 55), ('case', 54), ('plan', 54), ('claim', 54), ('tell', 54), ('help', 53), ('news', 52), ('well', 52), ('former', 52), ('national', 52), ('need', 51), ('Ukraine', 51), ('government', 51), ('October', 51), ('season', 51), ('quarterback', 51), ('country', 50), ('know', 49), ('full', 49), ('member', 49), ('global', 49), ('China', 49), ('something', 48), ('church', 48), ('good', 47), ('place', 47), ('Video', 47), ('statement', 46), ('trial', 46), ('long', 46), ('mean', 46), ('mother', 45), ('end', 45), ('human', 45), ('different', 45), ('never', 44), ('island', 44), ('foreign', 44), ('parent', 44), ('minister', 44), ('money', 43), ('system', 43)]\n"
     ]
    }
   ],
   "source": [
    "# 품사 태깅: 단어(token)에 품사를 붙여주는 작업 \n",
    "# 품사를 보고 단어 학습에서 제외할 품사(ex, 조사, 접속사..)의 단어들은 제거하기 위해\n",
    "EXCLUDE = [\"CC\", \"CD\", \"DT\", \"EX\", \"IN\", \"MD\", \"POS\", \"PRP\", \"PRP$\", \"UH\", \"VBD\", \"VBG\", \"VBN\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "\n",
    "tags = nltk.pos_tag(lemm_tokens)\n",
    "word_dict = {}\n",
    "for word, tag in tags:\n",
    "    if word.isalpha() and len(word) > 2 and tag not in EXCLUDE: # 영어이고, 1글자이상이며, 제외 품사에 해당되지 않을 때 \n",
    "        if word_dict.get(word) is None:\n",
    "            word_dict[word] = 1\n",
    "        else:\n",
    "            word_dict[word] += 1  # 같은 단어를 results dictionary에 넣을 경우 빈도 수 +1 \n",
    "\n",
    "word_list = sorted(word_dict.items(), key=lambda item: item[1], reverse=True) # 빈도수 순서대로 단어 dictionary 정렬\n",
    "words = [x[0] for x in word_list] # 오직 단어만 담은 list \n",
    "print(\"크롤링된 단어 사전 길이: \", len(word_list))\n",
    "print(word_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d552c",
   "metadata": {},
   "source": [
    "# 3. Translation with Daum Dictionary \n",
    "포털사이트 다음(Daum)에서 찾고자 하는 키워드를 전달하면 한글 뜻과 영어 뜻을 반환하는 함수(get_mean)를 작성하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "db15e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "def get_mean(search_keyword):\n",
    "    # daum 사전 사이트에 접근\n",
    "    url = 'https://alldic.daum.net/search.do?q=' + search_keyword\n",
    "    web = urlopen(url) \n",
    "    web_page = BeautifulSoup(web, 'html.parser')\n",
    "    \n",
    "    # daum 사전 사이트에 찾고자 하는 키워드(search_keyword)를 입력하고 나온 결과창의 요소들에 접근\n",
    "    mean_data = []\n",
    "    cnt = 0 \n",
    "    for block in web_page.find_all('div', {'class': 'card_word'}): # 결과창 요소 = block \n",
    "        mean = block.find('ul', {'class': 'list_search'}).get_text().replace(\"\\n\", \" \")\n",
    "        mean_data.append(mean)\n",
    "        cnt += 1 \n",
    "        if cnt == 2: break # 한글, 영어 뜻까지만 찾고 for문 탈출\n",
    "    \n",
    "    return mean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "66e3ed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan\n",
      "[' 1.계획하다 2.예정 3.플랜 4.구상 5.의도 ', ' a series of steps to be carried out or goals to be accomplished ']\n"
     ]
    }
   ],
   "source": [
    "# 테스트를 위해 작성된 코드\n",
    "# 랜덤 함수를 이용하였기 때문에 해당 셀을 여러번 실행해도 다른 결과값이 나오고 이로부터 테스트 결과를 강건하게 확인 가능 \n",
    "rand = random.randint(0, 100)\n",
    "test_word = word_list[rand][0]\n",
    "print(test_word)\n",
    "print(get_mean(test_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7a2845",
   "metadata": {},
   "source": [
    "# 4. English Word Test Program\n",
    "해외 매체에서 크롤링한 단어로 시험을 보는 프로그램을 아래에서 최종 작성하였다. \n",
    "1. 한영, 영영 모드 중 선택하여 테스트 가능 \n",
    "2. 한 문제 당 총 n번(chance)의 기회 제공\n",
    "3. 기회 횟수가 3회 이하로 떨어질 경우 hint 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1fc0530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_hint(word, chance):\n",
    "    chars = [c for c in word]                 # 단어를 문자 단위로 저장 (ex, words -> w o r d s)\n",
    "    hcnt = int(len(chars) * (1 - 0.5/chance)) # 남은 기회(chance) 비율에 따라 힌트로 제공할 문자 수를 조정함 \n",
    "    \n",
    "    rand = random.sample(range(0, len(chars)-1), hcnt)\n",
    "    for r in range(len(rand)): # 남은 기회 비율에 따라 랜덤으로 가릴 문자 선택 \n",
    "        chars[rand[r]] = \"_\"\n",
    "        \n",
    "    return print(f'힌트: {\" \".join(chars)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ac7771c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_test(WORDS_LIST, chance): \n",
    "    if len(WORDS_LIST) <= 0 : print(\"단어 데이터가 수집되어 있지 않습니다. 데이터를 수집하고 시작해주세요.\") # error handling\n",
    "    random.shuffle(WORDS_LIST)\n",
    "    \n",
    "    # 테스트 모드 선택 \n",
    "    MODE = int(input(\"진행할 단어 테스트의 모드를 입력하세요. \\n1) 한영 테스트 \\t 2) 영영 테스트 \\n\\n \"))\n",
    "    # 단어 테스트 시작\n",
    "    for i, word in enumerate(WORDS_LIST):\n",
    "        mean = get_mean(word)[MODE-1]\n",
    "        print(\"\\n\", mean) \n",
    "        CHANCE = chance\n",
    "        # 기회(chance) 안에 정답을 맞추도록 구성\n",
    "        while CHANCE > 0:\n",
    "            print(\"*\" * 80)\n",
    "            user = input(\"뜻의 단어를 입력하세요(종료는 'Q' 입력) \\n>>>>> \").strip()\n",
    "            if user == \"q\" or user == \"Q\": \n",
    "                return 0\n",
    "            if word.lower() == user.lower(): \n",
    "                print(\"정답!\")\n",
    "                break\n",
    "            else:\n",
    "                CHANCE -= 1\n",
    "                if CHANCE == 0: \n",
    "                    print(f\"정답은 {word} 입니다. \\n\\n\")\n",
    "                    break\n",
    "                print(\"\\n틀렸습니다. 남은 기회 \", str(CHANCE), \"회..\")\n",
    "                if CHANCE <= 3: show_hint(word, CHANCE) # 기회가 3회 이하로 떨어질 경우 힌트 제공 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e8713dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "진행할 단어 테스트의 모드를 입력하세요. \n",
      "1) 한영 테스트 \t 2) 영영 테스트 \n",
      "\n",
      " 1\n",
      "\n",
      "  1.차별하다 2.식별하다 \n",
      "********************************************************************************\n",
      "뜻의 단어를 입력하세요(종료는 'Q' 입력) \n",
      ">>>>> discriminate\n",
      "정답!\n",
      "\n",
      "  1.표제 2.자막 3.타이틀 \n",
      "********************************************************************************\n",
      "뜻의 단어를 입력하세요(종료는 'Q' 입력) \n",
      ">>>>> title\n",
      "\n",
      "틀렸습니다. 남은 기회  4 회..\n",
      "********************************************************************************\n",
      "뜻의 단어를 입력하세요(종료는 'Q' 입력) \n",
      ">>>>> index\n",
      "\n",
      "틀렸습니다. 남은 기회  3 회..\n",
      "힌트: _ a _ _ _ _ n\n",
      "********************************************************************************\n",
      "뜻의 단어를 입력하세요(종료는 'Q' 입력) \n",
      ">>>>> an\n",
      "\n",
      "틀렸습니다. 남은 기회  2 회..\n",
      "힌트: c _ _ _ _ _ n\n",
      "********************************************************************************\n",
      "뜻의 단어를 입력하세요(종료는 'Q' 입력) \n",
      ">>>>> caland\n",
      "\n",
      "틀렸습니다. 남은 기회  1 회..\n",
      "힌트: c a p _ _ _ n\n",
      "********************************************************************************\n",
      "뜻의 단어를 입력하세요(종료는 'Q' 입력) \n",
      ">>>>> caption\n",
      "정답!\n",
      "\n",
      "  굽다 \n",
      "********************************************************************************\n",
      "뜻의 단어를 입력하세요(종료는 'Q' 입력) \n",
      ">>>>> Q\n",
      "\n",
      "\n",
      "프로그램을 종료합니다...\n"
     ]
    }
   ],
   "source": [
    "result = word_test(words, 5) # 한 문제 당 주어지는 기회는 직접 설정 가능\n",
    "if result == 0 : print(\"\\n\\n프로그램을 종료합니다...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8917e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
